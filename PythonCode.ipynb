{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8065b6-4be8-4a8e-9623-9e5ea6f5f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.3.123-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (2.7.0)\n",
      "Collecting scipy>=1.4.1\n",
      "  Using cached scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Collecting matplotlib>=3.3.0\n",
      "  Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (2.2.5)\n",
      "Collecting seaborn>=0.11.0\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting pandas>=1.1.4\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Collecting torchvision>=0.9.0\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "Collecting ultralytics-thop>=2.0.0\n",
      "  Using cached ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (11.2.1)\n",
      "Collecting tqdm>=4.64.0\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: psutil in d:\\yolo-food classification\\myenv\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Collecting opencv-python>=4.6.0\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: fsspec in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
      "Requirement already satisfied: jinja2 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: networkx in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: filelock in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\yolo-food classification\\myenv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\yolo-food classification\\myenv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Installing collected packages: pandas, matplotlib, ultralytics-thop, tqdm, torchvision, seaborn, scipy, py-cpuinfo, opencv-python, ultralytics\n",
      "Successfully installed matplotlib-3.10.1 opencv-python-4.11.0.86 pandas-2.2.3 py-cpuinfo-9.0.0 scipy-1.15.2 seaborn-0.13.2 torchvision-0.22.0 tqdm-4.67.1 ultralytics-8.3.123 ultralytics-thop-2.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'D:\\YOLO-FOOD CLASSIFICATION\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138605e4-e962-45ff-9d97-44c84cd3a201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-cuda-runtime-cu12\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.9.37-py3-none-win_amd64.whl (3.6 MB)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu12\n",
      "Successfully installed nvidia-cuda-runtime-cu12-12.9.37\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'D:\\YOLO-FOOD CLASSIFICATION\\myenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install nvidia-cuda-runtime-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2dd971-679d-4ca8-9535-8cb4326ecfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d38e744-db4e-4067-b0a9-190bff28f986",
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e0ed4e-b56e-4c1c-be24-7c3bb205a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.123  Python-3.10.2 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolo11s-cls.pt, data=custom_dataset, epochs=8, time=None, patience=100, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train4\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\train... found 9866 images in 11 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\val... found 3430 images in 11 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 10                  -1  1    672011  ultralytics.nn.modules.head.Classify         [512, 11]                     \n",
      "YOLO11s-cls summary: 86 layers, 5,457,099 parameters, 5,457,099 gradients, 12.1 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 4.31.2 MB/s, size: 59.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\train... 9866 images, 0 corrupt: 100%|██████████| 9866/9866 \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\train\\Dairy product\\332.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.60.9 ms, read: 5.72.7 MB/s, size: 44.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\YOLO-FOOD CLASSIFICATION\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\val... 3430 images, 0 corrupt: 100%|██████████| 3430/3430 [00:\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\val\\Dairy product\\6.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\YOLO-FOOD CLASSIFICATION\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sizes 320 train, 320 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train4\u001b[0m\n",
      "Starting training for 8 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/8         0G      1.123         10        320: 100%|██████████| 617/617 [18:56<00:00,  1.84s/it]  \n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:25<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.856       0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/8         0G     0.6135         10        320: 100%|██████████| 617/617 [20:07<00:00,  1.96s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:24<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.858       0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/8         0G     0.5525         10        320: 100%|██████████| 617/617 [19:23<00:00,  1.89s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:11<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.867       0.99\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/8         0G     0.4695         10        320: 100%|██████████| 617/617 [20:49<00:00,  2.02s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:31<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.881      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/8         0G     0.3912         10        320: 100%|██████████| 617/617 [20:56<00:00,  2.04s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:32<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.896      0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        6/8         0G     0.3103         10        320: 100%|██████████| 617/617 [19:32<00:00,  1.90s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [01:49<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.906      0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        7/8         0G     0.2364         10        320: 100%|██████████| 617/617 [16:56<00:00,  1.65s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:07<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       0.92      0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        8/8         0G     0.1824         10        320: 100%|██████████| 617/617 [17:42<00:00,  1.72s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [02:08<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.924      0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8 epochs completed in 2.879 hours.\n",
      "Optimizer stripped from runs\\classify\\train4\\weights\\last.pt, 11.0MB\n",
      "Optimizer stripped from runs\\classify\\train4\\weights\\best.pt, 11.0MB\n",
      "\n",
      "Validating runs\\classify\\train4\\weights\\best.pt...\n",
      "Ultralytics 8.3.123  Python-3.10.2 torch-2.7.0+cpu CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "YOLO11s-cls summary (fused): 47 layers, 5,448,219 parameters, 0 gradients, 12.0 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\train... found 9866 images in 11 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m D:\\YOLO-FOOD CLASSIFICATION\\custom_dataset\\val... found 3430 images in 11 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 108/108 [01:57<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.924      0.997\n",
      "Speed: 0.0ms preprocess, 24.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "createdmodel=YOLO(\"yolo11s-cls.pt\")\n",
    "results= createdmodel.train(data=\"custom_dataset\",epochs=8,imgsz=320 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cfa03-2782-4986-83eb-6330c9d2cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING PRETRAINED MODEL\n",
    "model_test=YOLO(\"runs/classify/train4/weights/best.pt\")\n",
    "results=model_test(\"test_images\", save=True, imgsz=320, conf=0.7)\n",
    "results[0].show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
